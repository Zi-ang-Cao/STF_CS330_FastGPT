{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw5ygzTXOjN1",
        "outputId": "616ec706-3d66-4e94-ba56-7db88ffc10e9"
      },
      "outputs": [],
      "source": [
        "# prompt: install datasets\n",
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk48QDKGP78i",
        "outputId": "23f949f0-2ba2-4451-b963-6fef0121fb1b"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch numpy tqdm openai nltk matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %conda activate F_GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ii-9AgUnt0h"
      },
      "source": [
        "# Fast Detect GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ssCHJWvjFY7P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/peterwz/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "\n",
        "def from_pretrained(cls, model_name, kwargs, cache_dir):\n",
        "    local_path = os.path.join(cache_dir, 'local.' + model_name.replace(\"/\", \"_\"))\n",
        "    try:\n",
        "        obj = cls.from_pretrained(local_path, **kwargs)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        obj = cls.from_pretrained(model_name, **kwargs, cache_dir=cache_dir)\n",
        "        obj.save_pretrained(local_path)\n",
        "    return obj\n",
        "\n",
        "# predefined models\n",
        "model_fullnames = {  'gpt2': 'gpt2',\n",
        "                     'gpt2-xl': 'gpt2-xl',\n",
        "                     'opt-2.7b': 'facebook/opt-2.7b',\n",
        "                     'gpt-neo-2.7B': 'EleutherAI/gpt-neo-2.7B',\n",
        "                     'gpt-j-6B': 'EleutherAI/gpt-j-6B',\n",
        "                     'gpt-neox-20b': 'EleutherAI/gpt-neox-20b',\n",
        "                     'mgpt': 'sberbank-ai/mGPT',\n",
        "                     'pubmedgpt': 'stanford-crfm/pubmedgpt',\n",
        "                     'mt5-xl': 'google/mt5-xl',\n",
        "                     'llama-13b': 'huggyllama/llama-13b',\n",
        "                     'llama2-13b': 'TheBloke/Llama-2-13B-fp16',\n",
        "                     'bloom-7b1': 'bigscience/bloom-7b1',\n",
        "                     'opt-13b': 'facebook/opt-13b',\n",
        "                     }\n",
        "float16_models = ['gpt-j-6B', 'gpt-neox-20b', 'llama-13b', 'llama2-13b', 'bloom-7b1', 'opt-13b']\n",
        "\n",
        "def get_model_fullname(model_name):\n",
        "    return model_fullnames[model_name] if model_name in model_fullnames else model_name\n",
        "\n",
        "def load_model(model_name, device, cache_dir):\n",
        "    model_fullname = get_model_fullname(model_name)\n",
        "    print(f'Loading model {model_fullname}...')\n",
        "    model_kwargs = {}\n",
        "    if model_name in float16_models:\n",
        "        model_kwargs.update(dict(torch_dtype=torch.float16))\n",
        "    if 'gpt-j' in model_name:\n",
        "        model_kwargs.update(dict(revision='float16'))\n",
        "    model = from_pretrained(AutoModelForCausalLM, model_fullname, model_kwargs, cache_dir)\n",
        "    print('Moving model to GPU...', end='', flush=True)\n",
        "    start = time.time()\n",
        "    model.to(device)\n",
        "    print(f'DONE ({time.time() - start:.2f}s)')\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(model_name, for_dataset, cache_dir):\n",
        "    model_fullname = get_model_fullname(model_name)\n",
        "    optional_tok_kwargs = {}\n",
        "    if \"facebook/opt-\" in model_fullname:\n",
        "        print(\"Using non-fast tokenizer for OPT\")\n",
        "        optional_tok_kwargs['fast'] = False\n",
        "    if for_dataset in ['pubmed']:\n",
        "        optional_tok_kwargs['padding_side'] = 'left'\n",
        "    else:\n",
        "        optional_tok_kwargs['padding_side'] = 'right'\n",
        "    base_tokenizer = from_pretrained(AutoTokenizer, model_fullname, optional_tok_kwargs, cache_dir=cache_dir)\n",
        "    if base_tokenizer.pad_token_id is None:\n",
        "        base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
        "        if '13b' in model_fullname:\n",
        "            base_tokenizer.pad_token_id = 0\n",
        "    return base_tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7msivRWrn5Fx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "\n",
        "# 15 colorblind-friendly colors\n",
        "COLORS = [\"#0072B2\", \"#009E73\", \"#D55E00\", \"#CC79A7\", \"#F0E442\",\n",
        "            \"#56B4E9\", \"#E69F00\", \"#000000\", \"#0072B2\", \"#009E73\",\n",
        "            \"#D55E00\", \"#CC79A7\", \"#F0E442\", \"#56B4E9\", \"#E69F00\"]\n",
        "\n",
        "\n",
        "def get_roc_metrics(real_preds, sample_preds):\n",
        "    fpr, tpr, _ = roc_curve([0] * len(real_preds) + [1] * len(sample_preds), real_preds + sample_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return fpr.tolist(), tpr.tolist(), float(roc_auc)\n",
        "\n",
        "\n",
        "def get_precision_recall_metrics(real_preds, sample_preds):\n",
        "    precision, recall, _ = precision_recall_curve([0] * len(real_preds) + [1] * len(sample_preds),\n",
        "                                                  real_preds + sample_preds)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    return precision.tolist(), recall.tolist(), float(pr_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iq4ITGPMn9eF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import argparse\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c4GxkxIkoGud"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_samples(logits, labels):\n",
        "    assert logits.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "    nsamples = 10000\n",
        "    lprobs = torch.log_softmax(logits, dim=-1)\n",
        "    distrib = torch.distributions.categorical.Categorical(logits=lprobs)\n",
        "    samples = distrib.sample([nsamples]).permute([1, 2, 0])\n",
        "    return samples\n",
        "\n",
        "def get_likelihood(logits, labels):\n",
        "    assert logits.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "    labels = labels.unsqueeze(-1) if labels.ndim == logits.ndim - 1 else labels\n",
        "    lprobs = torch.log_softmax(logits, dim=-1)\n",
        "    log_likelihood = lprobs.gather(dim=-1, index=labels)\n",
        "    return log_likelihood.mean(dim=1)\n",
        "\n",
        "def get_sampling_discrepancy(logits_ref, logits_score, labels):\n",
        "    assert logits_ref.shape[0] == 1\n",
        "    assert logits_score.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "    if logits_ref.size(-1) != logits_score.size(-1):\n",
        "        # print(f\"WARNING: vocabulary size mismatch {logits_ref.size(-1)} vs {logits_score.size(-1)}.\")\n",
        "        vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n",
        "        logits_ref = logits_ref[:, :, :vocab_size]\n",
        "        logits_score = logits_score[:, :, :vocab_size]\n",
        "\n",
        "    samples = get_samples(logits_ref, labels)\n",
        "    log_likelihood_x = get_likelihood(logits_score, labels)\n",
        "    log_likelihood_x_tilde = get_likelihood(logits_score, samples)\n",
        "    miu_tilde = log_likelihood_x_tilde.mean(dim=-1)\n",
        "    sigma_tilde = log_likelihood_x_tilde.std(dim=-1)\n",
        "    discrepancy = (log_likelihood_x.squeeze(-1) - miu_tilde) / sigma_tilde\n",
        "    return discrepancy.item()\n",
        "\n",
        "def get_sampling_discrepancy_analytic(logits_ref, logits_score, labels):\n",
        "    assert logits_ref.shape[0] == 1\n",
        "    assert logits_score.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "    if logits_ref.size(-1) != logits_score.size(-1):\n",
        "        # print(f\"WARNING: vocabulary size mismatch {logits_ref.size(-1)} vs {logits_score.size(-1)}.\")\n",
        "        vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n",
        "        logits_ref = logits_ref[:, :, :vocab_size]\n",
        "        logits_score = logits_score[:, :, :vocab_size]\n",
        "\n",
        "    labels = labels.unsqueeze(-1) if labels.ndim == logits_score.ndim - 1 else labels\n",
        "    lprobs_score = torch.log_softmax(logits_score, dim=-1)\n",
        "    probs_ref = torch.softmax(logits_ref, dim=-1)\n",
        "    log_likelihood = lprobs_score.gather(dim=-1, index=labels).squeeze(-1)\n",
        "    mean_ref = (probs_ref * lprobs_score).sum(dim=-1)\n",
        "    var_ref = (probs_ref * torch.square(lprobs_score)).sum(dim=-1) - torch.square(mean_ref)\n",
        "    discrepancy = (log_likelihood.sum(dim=-1) - mean_ref.sum(dim=-1)) / var_ref.sum(dim=-1).sqrt()\n",
        "    discrepancy = discrepancy.mean()\n",
        "    return discrepancy.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yv3QCqJLoJB5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "import json\n",
        "import transformers\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZT_Rz4qpSa-",
        "outputId": "ca236718-c4a8-4c38-a216-c7d9c38fb4ff"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/baoguangsheng/fast-detect-gpt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FROd1DwYpdMw",
        "outputId": "05472a85-b36b-47d6-8ecc-0e14352094a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/disks/disk/CS330/STF_CS330_FastGPT/private_support_code/fast-detect-gpt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/peterwz/.local/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "# %cd fast-detect-gpt\n",
        "%cd /home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/private_support_code/fast-detect-gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8N6RGGvpk0R",
        "outputId": "e922a425-7482-4687-c694-e4b2961c631f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[34;42mCS330_FastGPT_med_cuda\u001b[0m/  \u001b[01;32mREADME.md\u001b[0m*         \u001b[34;42mlocal_infer_ref\u001b[0m/   \u001b[01;32mtemperature.sh\u001b[0m*\n",
            "\u001b[01;32mFirstAgent.zip\u001b[0m*          \u001b[34;42m__pycache__\u001b[0m/       \u001b[01;32mmain.sh\u001b[0m*           \u001b[34;42mtensorboard_log\u001b[0m/\n",
            "\u001b[01;32mGPT_NEW_ACT.py\u001b[0m*          \u001b[01;32mattack.sh\u001b[0m*         \u001b[01;32mmain_ext.sh\u001b[0m*       \u001b[01;32mtopk.sh\u001b[0m*\n",
            "\u001b[01;32mGPT_NEW_ACT_A2C.py\u001b[0m*      \u001b[01;32mcommon_helper.py\u001b[0m*  \u001b[01;32mrequirements.txt\u001b[0m*  \u001b[01;32mtopp.sh\u001b[0m*\n",
            "\u001b[01;32mGPT_NEW_ACT_DQN.py\u001b[0m*      \u001b[34;42mexp_gpt3to4\u001b[0m/       \u001b[34;42mscripts\u001b[0m/\n",
            "\u001b[01;32mGPT_OLD_ACT.py\u001b[0m*          \u001b[34;42mexp_main\u001b[0m/          \u001b[01;32msetup.sh\u001b[0m*\n",
            "\u001b[01;32mLICENSE\u001b[0m*                 \u001b[01;32mgpt3to4.sh\u001b[0m*        \u001b[01;32msupervised.sh\u001b[0m*\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lyAErSoqobFT"
      },
      "outputs": [],
      "source": [
        "# reference_model_name = \"gpt-j-6B\"\n",
        "# scoring_model_name = \"gpt-neo-2.7B\"\n",
        "\n",
        "reference_model_name = \"gpt2\"\n",
        "scoring_model_name = \"gpt2\"\n",
        "\n",
        "\n",
        "dataset = \"xsum\"\n",
        "ref_path = \"./local_infer_ref\"\n",
        "device = \"cuda\"\n",
        "cache_dir = \"../cache\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "S2-ooCqSoMzj"
      },
      "outputs": [],
      "source": [
        "class ProbEstimator:\n",
        "    def __init__(self):\n",
        "        self.real_crits = []\n",
        "        self.fake_crits = []\n",
        "        for result_file in glob.glob(os.path.join(ref_path, '*.json')):\n",
        "            with open(result_file, 'r') as fin:\n",
        "                res = json.load(fin)\n",
        "                self.real_crits.extend(res['predictions']['real'])\n",
        "                self.fake_crits.extend(res['predictions']['samples'])\n",
        "        print(f'ProbEstimator: total {len(self.real_crits) * 2} samples.')\n",
        "\n",
        "\n",
        "    def crit_to_prob(self, crit):\n",
        "        offset = np.sort(np.abs(np.array(self.real_crits + self.fake_crits) - crit))[100]\n",
        "        cnt_real = np.sum((np.array(self.real_crits) > crit - offset) & (np.array(self.real_crits) < crit + offset))\n",
        "        cnt_fake = np.sum((np.array(self.fake_crits) > crit - offset) & (np.array(self.fake_crits) < crit + offset))\n",
        "        return cnt_fake / (cnt_real + cnt_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "u1vtHhL9pwkC"
      },
      "outputs": [],
      "source": [
        "class FastDetectGPT:\n",
        "    def __init__(self):\n",
        "        self.device = device\n",
        "        # load model\n",
        "        self.scoring_tokenizer = load_tokenizer(scoring_model_name, dataset, cache_dir)\n",
        "        self.scoring_model = load_model(scoring_model_name, device, cache_dir)\n",
        "        self.scoring_model.eval()\n",
        "        self.reference_model_name = reference_model_name\n",
        "        self.scoring_model_name = scoring_model_name\n",
        "        if self.reference_model_name != self.scoring_model_name:\n",
        "            self.reference_tokenizer = load_tokenizer(self.reference_model_name, dataset, cache_dir)\n",
        "            self.reference_model = load_model(self.reference_model_name, device, cache_dir)\n",
        "            self.reference_model.eval()\n",
        "        # evaluate criterion\n",
        "        self.criterion_name = \"sampling_discrepancy_analytic\"\n",
        "        self.criterion_fn = get_sampling_discrepancy_analytic\n",
        "        self.prob_estimator = ProbEstimator()\n",
        "        # input text\n",
        "        print('Local demo for Fast-DetectGPT, where the longer text has more reliable result.')\n",
        "        print('')\n",
        "\n",
        "    def infer(self, text):\n",
        "        # evaluate text     # (1, 112)\n",
        "        tokenized = self.scoring_tokenizer(text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(self.device)\n",
        "        labels = tokenized.input_ids[:, 1:]\n",
        "        with torch.no_grad():\n",
        "            logits_score = self.scoring_model(**tokenized).logits[:, :-1]\n",
        "            if self.reference_model_name == self.scoring_model_name:\n",
        "                logits_ref = logits_score\n",
        "            else:\n",
        "                tokenized = self.reference_tokenizer(text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(self.device)\n",
        "                assert torch.all(tokenized.input_ids[:, 1:] == labels), \"Tokenizer is mismatch.\"\n",
        "                logits_ref = self.reference_model(**tokenized).logits[:, :-1]\n",
        "            crit = self.criterion_fn(logits_ref, logits_score, labels)\n",
        "        # estimate the probability of machine generated text\n",
        "        prob = self.prob_estimator.crit_to_prob(crit)\n",
        "        print(f'Fast-DetectGPT criterion is {crit:.4f}, suggesting that the text has a probability of {prob * 100:.0f}% to be fake.')\n",
        "        return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOCRHiiDv1GR",
        "outputId": "003ffb07-c8c1-42a5-d289-8957f038275a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model gpt2...\n",
            "Moving model to GPU...DONE (1.73s)\n",
            "ProbEstimator: total 1800 samples.\n",
            "Local demo for Fast-DetectGPT, where the longer text has more reliable result.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "detector = FastDetectGPT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pyDRfVaiqMWl"
      },
      "outputs": [],
      "source": [
        "from typing import List, Set\n",
        "\n",
        "def model2hfname(model: str) -> str:\n",
        "    return {\n",
        "        \"bert-tiny\": \"prajjwal1/bert-tiny\",\n",
        "        \"bert-med\": \"prajjwal1/bert-medium\",\n",
        "        \"small\": \"gpt2\",\n",
        "        \"med\": \"gpt2-medium\",\n",
        "        \"large\": \"gpt2-large\",\n",
        "        \"full\": \"gpt2-xl\",\n",
        "        \"gpt2-sm\": \"gpt2\",\n",
        "        \"gpt2-med\": \"gpt2-medium\",\n",
        "        \"gpt2-lg\": \"gpt2-large\",\n",
        "        \"gpt2\": \"gpt2-xl\",\n",
        "        \"neo\": \"EleutherAI/gpt-neo-2.7B\",\n",
        "    }[model]\n",
        "\n",
        "def get_model_and_tokenizer(model: str, Cls = transformers.AutoModelForCausalLM, **model_kwargs):\n",
        "    hf_model_name = model2hfname(model)\n",
        "\n",
        "    m = Cls.from_pretrained(hf_model_name, **model_kwargs)\n",
        "    if isinstance(m, transformers.GPT2LMHeadModel):\n",
        "        m.transformer.gradient_checkpointing_enable()\n",
        "\n",
        "    tok = transformers.AutoTokenizer.from_pretrained(hf_model_name)\n",
        "\n",
        "    if tok.pad_token_id is None:\n",
        "        if Cls == transformers.AutoModelForCausalLM:\n",
        "            tok.pad_token = tok.eos_token\n",
        "        else:\n",
        "            print(\"Adding pad token to tokenizer\")\n",
        "            tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "            tok.pad_token = \"[PAD]\"\n",
        "    return m, tok\n",
        "\n",
        "\n",
        "def stop_tokens(tokenizer, stop_strings: Set[str] = set(\".\")) -> List[int]:\n",
        "    tokens = []\n",
        "    for idx in range(len(tokenizer)):\n",
        "        if tokenizer.decode(idx) in stop_strings:\n",
        "            tokens.append(idx)\n",
        "    print(\"Stop tokens:\", tokens)\n",
        "    return tokens\n",
        "\n",
        "def ignore_tokens(tokenizer, stop_strings: Set[str] = set(\"\\n\")) -> List[int]:\n",
        "    tokens = []\n",
        "    for idx in range(len(tokenizer)):\n",
        "        if tokenizer.decode(idx) in stop_strings:\n",
        "            tokens.append(idx)\n",
        "    print(\"Ignore tokens:\", tokens)\n",
        "    return tokens\n",
        "\n",
        "def ignore_tokens_replace(tokenizer, stop_strings: Set[str] = set(\" \")) -> List[int]:\n",
        "    tokens = []\n",
        "    for idx in range(len(tokenizer)):\n",
        "        if tokenizer.decode(idx) in stop_strings:\n",
        "            tokens.append(idx)\n",
        "    print(\"Ignore tokens replaced by:\", tokens)\n",
        "    return tokens[0]\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    values, _ = torch.topk(logits, k)\n",
        "    min_values = values[:, -1]\n",
        "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-iGXWboHqsgs"
      },
      "outputs": [],
      "source": [
        "max_sample_tokens = 200\n",
        "model_name = \"med\"\n",
        "env_device = \"cuda\"\n",
        "\n",
        "algorithm = \"PPO\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pGP7BMLMqAxO"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "# import gym\n",
        "\n",
        "\n",
        "class LMEnv(gym.Env):\n",
        "    def __init__(self, sampling_mode: str = \"likelihood\", topK_logistics: int=10, dataset: str=\"xsum\", n_train:int = 256, \n",
        "    random_seed:int=42, obs_dim:int = 10):\n",
        "\n",
        "        # Dataset\n",
        "        self.random_seed = random_seed\n",
        "        self.dataset = dataset\n",
        "        self.n_train = n_train\n",
        "        self._load_datasets()\n",
        "\n",
        "        ## LLM\n",
        "        self.max_sample_tokens = max_sample_tokens\n",
        "        self.model, self.tok = get_model_and_tokenizer(model_name)\n",
        "        assert isinstance(self.model, transformers.GPT2LMHeadModel)\n",
        "        self.model.to(env_device)\n",
        "        self.stop_tokens = stop_tokens(self.tok)\n",
        "        self.ignore_tokens = ignore_tokens(self.tok)\n",
        "        self.ignore_tokens_replace = ignore_tokens_replace(self.tok)\n",
        "        self._seed = None\n",
        "        self.vocab_size = len(self.tok)\n",
        "        # Current inputs and logits\n",
        "        self.initial_text = self._get_new_input()\n",
        "        self.past_kvs = None\n",
        "        self.topK_logistics = topK_logistics\n",
        "\n",
        "        self.sampling_mode = sampling_mode  # \"likelihood\" or \"argmax\"\n",
        "        self.purturb_mode = \"argmax\"\n",
        "        self.input_ids = None\n",
        "\n",
        "        ## RL: Basic Action Space and Obs Space\n",
        "        # The first integer can take values 0 or 1 (2 possibilities)\n",
        "        # The second integer can take values 1 to 10 (10 possibilities)\n",
        "        # self.action_space = gym.spaces.MultiDiscrete([2, self.topK_logistics])\n",
        "        # self.action_space = gym.spaces.MultiDiscrete([self.topK_logistics])\n",
        "        self.obs_dim = obs_dim\n",
        "        self.action_space = gym.spaces.MultiDiscrete([2, self.topK_logistics])\n",
        "\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_dim, self.topK_logistics), dtype=np.float32)\n",
        "\n",
        "        from torch.utils.tensorboard import SummaryWriter\n",
        "        self.writer = SummaryWriter(f\"CS330_FastGPT_{model_name}_{env_device}/{algorithm}/OLD_Action\")\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    \n",
        "    def _feedforward(self, cur_input, past_kvs=None):\n",
        "        # Change 1: Speed up feedforward by utilizing past_kvs\n",
        "        \"\"\"\n",
        "        :param cur_input: When past_kvs = None, tensor shape [batch_size, seq_len]. When past_kvs is not None, tensor shape [batch_size, 1]\n",
        "        :param past_kvs: a cache to speed up model inference\n",
        "        :return returned_logits: tensor shape [batch_size, obs_dim, vocab_size] all logits up to the last point, clipped by obs_dim\n",
        "        :return new_past_kvs: the new model state cache\n",
        "        \"\"\"\n",
        "        # print(\"cur_input: \", cur_input.shape)\n",
        "        # if cur_input.shape[-1] ==0:\n",
        "        #     input()\n",
        "        with torch.inference_mode():\n",
        "            outputs = self.model(cur_input, past_key_values=past_kvs, use_cache=True)\n",
        "            all_logits = outputs.logits\n",
        "            B, S, V = all_logits.shape\n",
        "            returned_logits = torch.ones(B, self.obs_dim, V).float().to(env_device)\n",
        "            if S < self.obs_dim:\n",
        "                returned_logits[:, self.obs_dim - S:, :] = all_logits\n",
        "            else:\n",
        "                returned_logits = all_logits[:, S - self.obs_dim:, :]\n",
        "            new_past_kvs = outputs.past_key_values\n",
        "            return returned_logits, new_past_kvs\n",
        "\n",
        "    def _cat_new_word(self, sampled_token):\n",
        "        return torch.cat((self.input_ids, sampled_token.clone().detach().long().expand(1, 1)), dim=1)    \n",
        "    \n",
        "    def _sample_tokens(self, local_logits):\n",
        "        # Change 2: Return the new token as well as concatenated previous tokens\n",
        "        \"\"\"\n",
        "        :param local_logits: tensor shape [batch_size, vocab_size] local logits at the last point\n",
        "        :return new_token: works together with past_kvs returned from get_logits() to feed in the next round of get_logits().\n",
        "        :return new_input_ids: when past_kvs = None, this would return the complete input concat with output up to this point\n",
        "        \"\"\"\n",
        "        if self.sampling_mode == \"argmax\":\n",
        "            sampled_token = torch.argmax(local_logits, dim=-1)\n",
        "        elif self.sampling_mode == \"likelihood\":\n",
        "            # print(local_logits.shape, x.shape)\n",
        "            sampled_token = torch.multinomial(F.softmax(local_logits, dim=-1), num_samples=1).squeeze(dim=1)\n",
        "            # sampled_token = torch.multinomial(x, num_samples=1).squeeze(dim=1)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        \n",
        "        # Replace tokens such as new line with spaces\n",
        "        if sampled_token[0] in self.ignore_tokens:\n",
        "            sampled_token[0] = self.ignore_tokens_replace\n",
        "\n",
        "        new_token = sampled_token.unsqueeze(0)\n",
        "        new_input_ids = self._cat_new_word(new_token)\n",
        "        return new_token, new_input_ids\n",
        "         \n",
        "    def _perturb_tokens(self, local_logits, perturb_ranking):\n",
        "        \"\"\"\n",
        "        :param local_logits: tensor shape [batch_size, vocab_size] local logits at the last point\n",
        "        :param perturb_ranking: perturb selection of the last word\n",
        "        :return new_token: the selected token to generate\n",
        "        :return new_input_ids: the new input ids after the perturbation\n",
        "        \"\"\"\n",
        "        # Get the top k predictions （1-10）\n",
        "        _, topk_indices = torch.topk(local_logits, perturb_ranking)\n",
        "        # Select the last item\n",
        "        new_token = topk_indices[0][-1]\n",
        "        new_input_ids = self._cat_new_word(new_token)\n",
        "        return new_token, new_input_ids\n",
        "\n",
        "    def _obs_wrapper(self, all_logits):\n",
        "        # Sorted topk_values\n",
        "        # TODO(ziangcao): add previous model parts to the observation\n",
        "        topk_values, _ = torch.topk(all_logits, self.topK_logistics, dim=-1)\n",
        "        # Normalize the topk_values\n",
        "        topk_values = F.softmax(topk_values, dim=-1)\n",
        "        # Remove batch dim\n",
        "        topk_values = topk_values.squeeze(dim=0)\n",
        "        return topk_values.detach().cpu().numpy()\n",
        "\n",
        "    def _load_datasets(self):\n",
        "        print(\"Dataset:\", self.dataset)\n",
        "        if self.dataset == \"xsum\":\n",
        "            d = datasets.load_dataset(self.dataset, split=\"train\").shuffle(seed=self.random_seed)\n",
        "            filter_fn = lambda rows: [\n",
        "                len(a.split(\" \")) < 100 for a in rows[\"document\"]\n",
        "            ]\n",
        "            d = d.filter(filter_fn, batched=True, batch_size=None)\n",
        "            d = d[\"document\"][:self.n_train]\n",
        "            self.data = d\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def _get_new_input(self):\n",
        "        return self.data[np.random.randint(self.n_train)].replace('\\n', ' ')\n",
        "    \n",
        "    def _sample_done(self):\n",
        "        a = self.input_ids[0][-1] in self.stop_tokens\n",
        "        b = self.input_ids.shape[1] >= self.max_sample_tokens\n",
        "        return a or b\n",
        "\n",
        "    def reset(self, seed: int = None):\n",
        "        print(\"Resetting environment=============\")\n",
        "        ## Get a new generate starting point\n",
        "        initial_text = self._get_new_input()\n",
        "        self.input_ids = self.tok(initial_text, return_tensors=\"pt\")[\"input_ids\"].to(env_device)\n",
        "        while self.input_ids.shape[-1] ==0:\n",
        "            initial_text = self._get_new_input()\n",
        "            self.input_ids = self.tok(initial_text, return_tensors=\"pt\")[\"input_ids\"].to(env_device)\n",
        "        ## First 1 step\n",
        "        all_logits, new_past_kvs = self._feedforward(self.input_ids)\n",
        "        local_logits = all_logits[:, -1, :]\n",
        "        self.last_logits = local_logits\n",
        "        self.past_kvs = new_past_kvs\n",
        "\n",
        "        _, new_input_ids = self._sample_tokens(local_logits)\n",
        "        self.input_ids = new_input_ids\n",
        "\n",
        "        obs = self._obs_wrapper(all_logits)\n",
        "\n",
        "        # reset_info = None  # or reset_info = {} if you prefer\n",
        "        reset_info = {\"TimeLimit.truncated\": False,}  # or reset_info = {} if you prefer\n",
        "        return obs, reset_info\n",
        "        # return obs\n",
        "\n",
        "    def get_text(self):\n",
        "        return self.tok.decode(torch.squeeze(self.input_ids, dim=0))\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0.\n",
        "        # Parse Action\n",
        "        perturb = action[0]\n",
        "        ## perturb_ranking: 10 options -- shift the choice from 0-9 toward 1-10\n",
        "        perturb_ranking = action[1] + 1\n",
        "\n",
        "        # ## perturb_ranking: 10 options -- shift the choice from 0-9 toward 1-10\n",
        "        # perturb_ranking = action[1] + 1\n",
        "\n",
        "        sampled_token, sampled_output = self._sample_tokens(self.last_logits)\n",
        "\n",
        "        if not perturb:\n",
        "            self.input_ids = sampled_output\n",
        "            cur_input = sampled_token\n",
        "            # print(\"Raw: \", reward)\n",
        "            prob_drop = 0.\n",
        "            sampled_score = 0.\n",
        "            perturbed_score = 0.\n",
        "        else:\n",
        "            reward -= 1. # Cost of applying perturb\n",
        "            # TODO(ziangcao): better give a large value instead of 1\n",
        "            _, perturbed_output = self._perturb_tokens(self.last_logits, perturb_ranking)\n",
        "\n",
        "            # Record Scores -- prob\n",
        "            print()\n",
        "            sampled_score = detector.infer(self.tok.decode(torch.squeeze(sampled_output, dim=0)))\n",
        "            perturbed_score = detector.infer(self.tok.decode(torch.squeeze(perturbed_output, dim=0)))\n",
        "\n",
        "            assert sampled_score>=0\n",
        "            assert perturbed_score>=0\n",
        "\n",
        "            reward += (sampled_score-perturbed_score) * 100. # Benefits of applying perturb\n",
        "\n",
        "            self.input_ids = perturbed_output\n",
        "            cur_input = self.input_ids\n",
        "            self.past_kvs = None\n",
        "\n",
        "            # print(\"Perturbed: \", reward)\n",
        "        \n",
        "\n",
        "        idx = self.input_ids.shape[1]\n",
        "        prob_drop = sampled_score-perturbed_score\n",
        "\n",
        "        self.writer.add_scalar(\"perturb\", perturb, idx)\n",
        "        self.writer.add_scalar(\"reward\", reward, idx)\n",
        "        self.writer.add_scalar(\"Prob_Drop\", prob_drop, idx)\n",
        "        self.writer.add_scalar(\"sampled_score\", sampled_score, idx)\n",
        "        self.writer.add_scalar(\"perturbed_score\", perturbed_score, idx)\n",
        "\n",
        "\n",
        "        ## GET NEW OBS\n",
        "        all_logits, new_past_kvs = self._feedforward(cur_input, self.past_kvs)\n",
        "        local_logits = all_logits[:, -1, :]\n",
        "        self.last_logits = local_logits\n",
        "        self.past_kvs = new_past_kvs\n",
        "\n",
        "        obs = self._obs_wrapper(all_logits)\n",
        "\n",
        "        info = {\"TimeLimit.truncated\": False,}\n",
        "\n",
        "        done = self._sample_done()\n",
        "\n",
        "        # If your environment does not have a concept of truncation, you can set truncated to the same value as done\n",
        "        truncated = done\n",
        "        return obs, reward, done, truncated, info\n",
        "        # return obs, reward, done, info\n",
        "\n",
        "    \n",
        "    def seed(self, seed=None):\n",
        "        self._seed = seed\n",
        "        pass\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, SAC\n",
        "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, BaseCallback\n",
        "from stable_baselines3.common.utils import obs_as_tensor, safe_mean, set_random_seed\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env.subproc_vec_env import  SubprocVecEnv, _flatten_obs\n",
        "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
        "\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "def init_env_for_agent_training(n_envs: int=1):\n",
        "    def make_env():\n",
        "        def _make_env():\n",
        "            env=LMEnv()\n",
        "            check_env(env)\n",
        "\n",
        "            return env\n",
        "        \n",
        "        if n_envs == -1:\n",
        "            return _make_env()\n",
        "        else:\n",
        "            return _make_env()\n",
        "\n",
        "    if n_envs == -1:\n",
        "        return make_env()\n",
        "    if n_envs == 1:\n",
        "        return DummyVecEnv([make_env for _ in range(n_envs)])\n",
        "    else:\n",
        "        return SubprocVecEnv([make_env for _ in range(n_envs)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: xsum\n",
            "Stop tokens: [13, 764]\n",
            "Ignore tokens: [198]\n",
            "Ignore tokens replaced by: [220]\n",
            "Resetting environment=============\n",
            "Resetting environment=============\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is 0.2211, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3144, suggesting that the text has a probability of 15% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is -0.5149, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -0.5284, suggesting that the text has a probability of 3% to be fake.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ziangcao2022/workspace/miniconda3/envs/F_GPT/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fast-DetectGPT criterion is -0.3197, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -0.3933, suggesting that the text has a probability of 2% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.2048, suggesting that the text has a probability of 3% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0838, suggesting that the text has a probability of 5% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.1472, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0262, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0139, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0597, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1518, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0561, suggesting that the text has a probability of 7% to be fake.\n"
          ]
        }
      ],
      "source": [
        "vec_env = init_env_for_agent_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Resetting environment=============\n",
            "Logging to ./tensorboard_log/old_ActionSpace_1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fast-DetectGPT criterion is 0.3550, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6357, suggesting that the text has a probability of 28% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6855, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5002, suggesting that the text has a probability of 19% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5640, suggesting that the text has a probability of 25% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5978, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5833, suggesting that the text has a probability of 25% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5456, suggesting that the text has a probability of 22% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5966, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5197, suggesting that the text has a probability of 20% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5989, suggesting that the text has a probability of 25% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5880, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5789, suggesting that the text has a probability of 25% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5699, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5701, suggesting that the text has a probability of 25% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5701, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5367, suggesting that the text has a probability of 20% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5013, suggesting that the text has a probability of 19% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4673, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6258, suggesting that the text has a probability of 27% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6044, suggesting that the text has a probability of 26% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6858, suggesting that the text has a probability of 27% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7722, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.9249, suggesting that the text has a probability of 36% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.8702, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 1.0286, suggesting that the text has a probability of 40% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.9520, suggesting that the text has a probability of 37% to be fake.\n",
            "Fast-DetectGPT criterion is 1.0529, suggesting that the text has a probability of 43% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.0399, suggesting that the text has a probability of 42% to be fake.\n",
            "Fast-DetectGPT criterion is 0.9823, suggesting that the text has a probability of 40% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6616, suggesting that the text has a probability of 29% to be fake.\n",
            "Fast-DetectGPT criterion is 0.9996, suggesting that the text has a probability of 41% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.0063, suggesting that the text has a probability of 43% to be fake.\n",
            "Fast-DetectGPT criterion is 1.0245, suggesting that the text has a probability of 41% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.9837, suggesting that the text has a probability of 40% to be fake.\n",
            "Fast-DetectGPT criterion is 1.1153, suggesting that the text has a probability of 46% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is -0.3232, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -0.2666, suggesting that the text has a probability of 1% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.2651, suggesting that the text has a probability of 1% to be fake.\n",
            "Fast-DetectGPT criterion is -0.2472, suggesting that the text has a probability of 1% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.1882, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is -0.1926, suggesting that the text has a probability of 4% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.1509, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is -0.2078, suggesting that the text has a probability of 3% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.2456, suggesting that the text has a probability of 1% to be fake.\n",
            "Fast-DetectGPT criterion is -0.1568, suggesting that the text has a probability of 5% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.2233, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -0.1503, suggesting that the text has a probability of 5% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0818, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0769, suggesting that the text has a probability of 5% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0002, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0002, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0214, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1711, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.2110, suggesting that the text has a probability of 10% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1155, suggesting that the text has a probability of 6% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1636, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0687, suggesting that the text has a probability of 6% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.2194, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0716, suggesting that the text has a probability of 6% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1305, suggesting that the text has a probability of 6% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2321, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1280, suggesting that the text has a probability of 6% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0102, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1487, suggesting that the text has a probability of 6% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1435, suggesting that the text has a probability of 6% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0540, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2567, suggesting that the text has a probability of 12% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3639, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3639, suggesting that the text has a probability of 14% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0273, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1964, suggesting that the text has a probability of 10% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1960, suggesting that the text has a probability of 10% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2040, suggesting that the text has a probability of 10% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0186, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2667, suggesting that the text has a probability of 13% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3362, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2149, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.5594, suggesting that the text has a probability of 4% to be fake.\n",
            "Fast-DetectGPT criterion is -0.3033, suggesting that the text has a probability of 1% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.3272, suggesting that the text has a probability of 1% to be fake.\n",
            "Fast-DetectGPT criterion is -0.3272, suggesting that the text has a probability of 1% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.4809, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -0.6524, suggesting that the text has a probability of 4% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.4801, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -0.6789, suggesting that the text has a probability of 4% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.8035, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -0.8952, suggesting that the text has a probability of 2% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -1.0258, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -0.9664, suggesting that the text has a probability of 2% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.8762, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -1.0468, suggesting that the text has a probability of 2% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is -1.6033, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -1.8277, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.1457, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -1.7228, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -1.7821, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.0169, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -1.8916, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -1.9465, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -1.8726, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.1499, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.2217, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.2772, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.4146, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4536, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.2966, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.3429, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.3168, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4196, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.2849, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4362, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.1856, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.1856, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.0932, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.2625, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.1772, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.1778, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.1685, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.2822, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.2445, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.3863, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.5741, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.3882, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.3909, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.3810, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.3948, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.5737, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.8753, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4873, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.5958, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4672, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.5578, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4529, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.6831, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.5832, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.6668, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.5907, suggesting that the text has a probability of 0% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is 1.6930, suggesting that the text has a probability of 68% to be fake.\n",
            "Fast-DetectGPT criterion is 1.7560, suggesting that the text has a probability of 76% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.8379, suggesting that the text has a probability of 88% to be fake.\n",
            "Fast-DetectGPT criterion is 1.6777, suggesting that the text has a probability of 68% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.5302, suggesting that the text has a probability of 56% to be fake.\n",
            "Fast-DetectGPT criterion is 1.7771, suggesting that the text has a probability of 81% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.7217, suggesting that the text has a probability of 73% to be fake.\n",
            "Fast-DetectGPT criterion is 1.8145, suggesting that the text has a probability of 85% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.9429, suggesting that the text has a probability of 87% to be fake.\n",
            "Fast-DetectGPT criterion is 1.9259, suggesting that the text has a probability of 88% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.6720, suggesting that the text has a probability of 66% to be fake.\n",
            "Fast-DetectGPT criterion is 1.9622, suggesting that the text has a probability of 86% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.9701, suggesting that the text has a probability of 86% to be fake.\n",
            "Fast-DetectGPT criterion is 1.9845, suggesting that the text has a probability of 87% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.7914, suggesting that the text has a probability of 83% to be fake.\n",
            "Fast-DetectGPT criterion is 1.9931, suggesting that the text has a probability of 86% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.6452, suggesting that the text has a probability of 63% to be fake.\n",
            "Fast-DetectGPT criterion is 1.9680, suggesting that the text has a probability of 86% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 2.0150, suggesting that the text has a probability of 85% to be fake.\n",
            "Fast-DetectGPT criterion is 2.0228, suggesting that the text has a probability of 85% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.9645, suggesting that the text has a probability of 86% to be fake.\n",
            "Fast-DetectGPT criterion is 1.6940, suggesting that the text has a probability of 69% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.7212, suggesting that the text has a probability of 73% to be fake.\n",
            "Fast-DetectGPT criterion is 1.4389, suggesting that the text has a probability of 53% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.4592, suggesting that the text has a probability of 55% to be fake.\n",
            "Fast-DetectGPT criterion is 1.3283, suggesting that the text has a probability of 48% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.3590, suggesting that the text has a probability of 53% to be fake.\n",
            "Fast-DetectGPT criterion is 1.3590, suggesting that the text has a probability of 53% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.1128, suggesting that the text has a probability of 47% to be fake.\n",
            "Fast-DetectGPT criterion is 1.3357, suggesting that the text has a probability of 48% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.2362, suggesting that the text has a probability of 45% to be fake.\n",
            "Fast-DetectGPT criterion is 1.1825, suggesting that the text has a probability of 48% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.2539, suggesting that the text has a probability of 45% to be fake.\n",
            "Fast-DetectGPT criterion is 1.1564, suggesting that the text has a probability of 43% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.1522, suggesting that the text has a probability of 44% to be fake.\n",
            "Fast-DetectGPT criterion is 1.1419, suggesting that the text has a probability of 45% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.2741, suggesting that the text has a probability of 46% to be fake.\n",
            "Fast-DetectGPT criterion is 1.3835, suggesting that the text has a probability of 53% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.3612, suggesting that the text has a probability of 52% to be fake.\n",
            "Fast-DetectGPT criterion is 1.1962, suggesting that the text has a probability of 46% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.1595, suggesting that the text has a probability of 43% to be fake.\n",
            "Fast-DetectGPT criterion is 1.2345, suggesting that the text has a probability of 46% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.2719, suggesting that the text has a probability of 45% to be fake.\n",
            "Fast-DetectGPT criterion is 1.3114, suggesting that the text has a probability of 49% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.3420, suggesting that the text has a probability of 49% to be fake.\n",
            "Fast-DetectGPT criterion is 1.2426, suggesting that the text has a probability of 45% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.0990, suggesting that the text has a probability of 45% to be fake.\n",
            "Fast-DetectGPT criterion is 1.3384, suggesting that the text has a probability of 49% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.3788, suggesting that the text has a probability of 52% to be fake.\n",
            "Fast-DetectGPT criterion is 1.4015, suggesting that the text has a probability of 51% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.4333, suggesting that the text has a probability of 52% to be fake.\n",
            "Fast-DetectGPT criterion is 1.3402, suggesting that the text has a probability of 49% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.3316, suggesting that the text has a probability of 48% to be fake.\n",
            "Fast-DetectGPT criterion is 1.2996, suggesting that the text has a probability of 47% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.2510, suggesting that the text has a probability of 45% to be fake.\n",
            "Fast-DetectGPT criterion is 1.1855, suggesting that the text has a probability of 48% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.0305, suggesting that the text has a probability of 41% to be fake.\n",
            "Fast-DetectGPT criterion is 1.0685, suggesting that the text has a probability of 43% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.1054, suggesting that the text has a probability of 45% to be fake.\n",
            "Fast-DetectGPT criterion is 0.9664, suggesting that the text has a probability of 39% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.0161, suggesting that the text has a probability of 42% to be fake.\n",
            "Fast-DetectGPT criterion is 0.9458, suggesting that the text has a probability of 38% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.0037, suggesting that the text has a probability of 43% to be fake.\n",
            "Fast-DetectGPT criterion is 1.0111, suggesting that the text has a probability of 42% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0509, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2532, suggesting that the text has a probability of 12% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0735, suggesting that the text has a probability of 6% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2690, suggesting that the text has a probability of 13% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3543, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2747, suggesting that the text has a probability of 13% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3579, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2592, suggesting that the text has a probability of 13% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.2612, suggesting that the text has a probability of 1% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0262, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0893, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0961, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3752, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1404, suggesting that the text has a probability of 6% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1455, suggesting that the text has a probability of 6% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0801, suggesting that the text has a probability of 5% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0803, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0312, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0262, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0262, suggesting that the text has a probability of 8% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is -2.2305, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -1.8683, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -1.7286, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -1.9287, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -1.9813, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4206, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.2180, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4553, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.7789, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4283, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.8929, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.5834, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.6124, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4956, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.7510, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.5766, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.5202, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.5146, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.1086, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.2139, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.0523, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4338, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.3418, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.4988, suggesting that the text has a probability of 0% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is 0.2567, suggesting that the text has a probability of 12% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1250, suggesting that the text has a probability of 6% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0648, suggesting that the text has a probability of 6% to be fake.\n",
            "Fast-DetectGPT criterion is -0.1749, suggesting that the text has a probability of 5% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.1773, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0507, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0643, suggesting that the text has a probability of 6% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0075, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0088, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0272, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.2852, suggesting that the text has a probability of 13% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2552, suggesting that the text has a probability of 12% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0892, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3658, suggesting that the text has a probability of 14% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4604, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.4604, suggesting that the text has a probability of 14% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5713, suggesting that the text has a probability of 25% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5403, suggesting that the text has a probability of 21% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4182, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5734, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6327, suggesting that the text has a probability of 28% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6194, suggesting that the text has a probability of 26% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6251, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 0.4414, suggesting that the text has a probability of 16% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is -0.1099, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0011, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0923, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1932, suggesting that the text has a probability of 9% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0002, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2507, suggesting that the text has a probability of 12% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1432, suggesting that the text has a probability of 6% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3380, suggesting that the text has a probability of 14% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3429, suggesting that the text has a probability of 15% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3430, suggesting that the text has a probability of 15% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4165, suggesting that the text has a probability of 15% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3086, suggesting that the text has a probability of 15% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4335, suggesting that the text has a probability of 15% to be fake.\n",
            "Fast-DetectGPT criterion is 0.4335, suggesting that the text has a probability of 15% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5347, suggesting that the text has a probability of 20% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3955, suggesting that the text has a probability of 16% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3199, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.4944, suggesting that the text has a probability of 16% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4253, suggesting that the text has a probability of 15% to be fake.\n",
            "Fast-DetectGPT criterion is 0.4932, suggesting that the text has a probability of 16% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4094, suggesting that the text has a probability of 15% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1279, suggesting that the text has a probability of 6% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0222, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3126, suggesting that the text has a probability of 15% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1992, suggesting that the text has a probability of 10% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2764, suggesting that the text has a probability of 13% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.2039, suggesting that the text has a probability of 10% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3145, suggesting that the text has a probability of 15% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4378, suggesting that the text has a probability of 15% to be fake.\n",
            "Fast-DetectGPT criterion is 0.4378, suggesting that the text has a probability of 15% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5915, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6938, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7718, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.7779, suggesting that the text has a probability of 23% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.8744, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 0.8354, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.9324, suggesting that the text has a probability of 37% to be fake.\n",
            "Fast-DetectGPT criterion is 0.8813, suggesting that the text has a probability of 30% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6652, suggesting that the text has a probability of 28% to be fake.\n",
            "Fast-DetectGPT criterion is 0.9245, suggesting that the text has a probability of 36% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.8980, suggesting that the text has a probability of 33% to be fake.\n",
            "Fast-DetectGPT criterion is 0.9183, suggesting that the text has a probability of 37% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.9383, suggesting that the text has a probability of 39% to be fake.\n",
            "Fast-DetectGPT criterion is 0.8130, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5917, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6809, suggesting that the text has a probability of 26% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6030, suggesting that the text has a probability of 26% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5721, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6844, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6727, suggesting that the text has a probability of 27% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7772, suggesting that the text has a probability of 23% to be fake.\n",
            "Fast-DetectGPT criterion is 0.7734, suggesting that the text has a probability of 23% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7828, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6265, suggesting that the text has a probability of 28% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7217, suggesting that the text has a probability of 23% to be fake.\n",
            "Fast-DetectGPT criterion is 0.7217, suggesting that the text has a probability of 23% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7859, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6094, suggesting that the text has a probability of 26% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6849, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 0.4809, suggesting that the text has a probability of 15% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4566, suggesting that the text has a probability of 15% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5324, suggesting that the text has a probability of 20% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7716, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.7948, suggesting that the text has a probability of 24% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7397, suggesting that the text has a probability of 22% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6801, suggesting that the text has a probability of 26% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6637, suggesting that the text has a probability of 29% to be fake.\n",
            "Fast-DetectGPT criterion is 0.7429, suggesting that the text has a probability of 24% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6892, suggesting that the text has a probability of 26% to be fake.\n",
            "Fast-DetectGPT criterion is 0.7641, suggesting that the text has a probability of 23% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6393, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 0.8222, suggesting that the text has a probability of 25% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is 1.0107, suggesting that the text has a probability of 42% to be fake.\n",
            "Fast-DetectGPT criterion is 1.0735, suggesting that the text has a probability of 45% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.4637, suggesting that the text has a probability of 55% to be fake.\n",
            "Fast-DetectGPT criterion is 1.1930, suggesting that the text has a probability of 47% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.1400, suggesting that the text has a probability of 44% to be fake.\n",
            "Fast-DetectGPT criterion is 1.2476, suggesting that the text has a probability of 45% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 1.2969, suggesting that the text has a probability of 47% to be fake.\n",
            "Fast-DetectGPT criterion is 1.2299, suggesting that the text has a probability of 46% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0370, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3520, suggesting that the text has a probability of 14% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3644, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2328, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1146, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1578, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1682, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1682, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3146, suggesting that the text has a probability of 15% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2358, suggesting that the text has a probability of 10% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3480, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2782, suggesting that the text has a probability of 13% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0894, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1461, suggesting that the text has a probability of 6% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0358, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3267, suggesting that the text has a probability of 14% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3941, suggesting that the text has a probability of 16% to be fake.\n",
            "Fast-DetectGPT criterion is 0.3701, suggesting that the text has a probability of 14% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4458, suggesting that the text has a probability of 17% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5648, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7481, suggesting that the text has a probability of 23% to be fake.\n",
            "Fast-DetectGPT criterion is 0.7160, suggesting that the text has a probability of 22% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7948, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6886, suggesting that the text has a probability of 27% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5288, suggesting that the text has a probability of 19% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5560, suggesting that the text has a probability of 23% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6697, suggesting that the text has a probability of 28% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5920, suggesting that the text has a probability of 24% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6538, suggesting that the text has a probability of 28% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6947, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7743, suggesting that the text has a probability of 23% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6861, suggesting that the text has a probability of 27% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.4749, suggesting that the text has a probability of 14% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6973, suggesting that the text has a probability of 24% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7182, suggesting that the text has a probability of 22% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6640, suggesting that the text has a probability of 29% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7034, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5373, suggesting that the text has a probability of 21% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.7154, suggesting that the text has a probability of 22% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6916, suggesting that the text has a probability of 26% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6261, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5822, suggesting that the text has a probability of 25% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6053, suggesting that the text has a probability of 26% to be fake.\n",
            "Fast-DetectGPT criterion is 0.6022, suggesting that the text has a probability of 26% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.3807, suggesting that the text has a probability of 16% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5334, suggesting that the text has a probability of 20% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.5916, suggesting that the text has a probability of 24% to be fake.\n",
            "Fast-DetectGPT criterion is 0.5391, suggesting that the text has a probability of 21% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.6112, suggesting that the text has a probability of 27% to be fake.\n",
            "Fast-DetectGPT criterion is 0.2361, suggesting that the text has a probability of 10% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0685, suggesting that the text has a probability of 5% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0382, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0158, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0548, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0804, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0274, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0487, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0231, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0567, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0335, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.0064, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is -0.0112, suggesting that the text has a probability of 8% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is -4.0431, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -4.0063, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -4.6847, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.0877, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -4.9924, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -4.9845, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.7237, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.4252, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.7088, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.4120, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.4583, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.1776, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.0877, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.1325, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.2395, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.0012, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.5543, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.2294, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.1928, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.1928, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.3168, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.7558, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.5754, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.7783, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.5672, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.6544, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.5499, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.5569, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.5161, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.6009, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.6519, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.5144, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.5506, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.5204, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.4115, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.5314, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.9071, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.4573, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.5084, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.4423, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.3346, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.2964, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.3150, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.1597, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -5.1391, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -5.1391, suggesting that the text has a probability of 0% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is 0.2366, suggesting that the text has a probability of 10% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0424, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.1807, suggesting that the text has a probability of 9% to be fake.\n",
            "Fast-DetectGPT criterion is 0.1741, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0007, suggesting that the text has a probability of 8% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0036, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0426, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0889, suggesting that the text has a probability of 8% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is 0.0176, suggesting that the text has a probability of 7% to be fake.\n",
            "Fast-DetectGPT criterion is 0.0933, suggesting that the text has a probability of 7% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.9020, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -0.6883, suggesting that the text has a probability of 4% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.6010, suggesting that the text has a probability of 4% to be fake.\n",
            "Fast-DetectGPT criterion is -0.8303, suggesting that the text has a probability of 2% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.5877, suggesting that the text has a probability of 4% to be fake.\n",
            "Fast-DetectGPT criterion is -0.4775, suggesting that the text has a probability of 2% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.4367, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -0.6776, suggesting that the text has a probability of 4% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.5881, suggesting that the text has a probability of 4% to be fake.\n",
            "Fast-DetectGPT criterion is -0.6336, suggesting that the text has a probability of 4% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -0.9768, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -1.1863, suggesting that the text has a probability of 1% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -1.0946, suggesting that the text has a probability of 2% to be fake.\n",
            "Fast-DetectGPT criterion is -1.3123, suggesting that the text has a probability of 0% to be fake.\n",
            "Resetting environment=============\n",
            "\n",
            "Fast-DetectGPT criterion is -2.2713, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.2033, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.7670, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.6805, suggesting that the text has a probability of 0% to be fake.\n",
            "\n",
            "Fast-DetectGPT criterion is -2.5773, suggesting that the text has a probability of 0% to be fake.\n",
            "Fast-DetectGPT criterion is -2.8306, suggesting that the text has a probability of 0% to be fake.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m algorithm\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, vec_env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, tensorboard_log\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./tensorboard_log\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m, tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mold_ActionSpace\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# model.save(\"FirstAgent\")\u001b[39;00m\n",
            "File \u001b[0;32m/home/ziangcao2022/workspace/miniconda3/envs/F_GPT/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
            "File \u001b[0;32m/home/ziangcao2022/workspace/miniconda3/envs/F_GPT/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/home/ziangcao2022/workspace/miniconda3/envs/F_GPT/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
            "File \u001b[0;32m/home/ziangcao2022/workspace/miniconda3/envs/F_GPT/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
            "File \u001b[0;32m/home/ziangcao2022/workspace/miniconda3/envs/F_GPT/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
            "\u001b[1;32m/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=220'>221</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mperturbed_score\u001b[39m\u001b[39m\"\u001b[39m, perturbed_score, idx)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=223'>224</a>\u001b[0m \u001b[39m## GET NEW OBS\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=224'>225</a>\u001b[0m all_logits, new_past_kvs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feedforward(cur_input, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpast_kvs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=225'>226</a>\u001b[0m local_logits \u001b[39m=\u001b[39m all_logits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=226'>227</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_logits \u001b[39m=\u001b[39m local_logits\n",
            "\u001b[1;32m/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# print(\"cur_input: \", cur_input.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# if cur_input.shape[-1] ==0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m#     input()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(cur_input, past_key_values\u001b[39m=\u001b[39;49mpast_kvs, use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     all_logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.125.194.68/home/peterwz/workspace/CS330/STF_CS330_FastGPT/src/gpt-old_peterwz.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     B, S, V \u001b[39m=\u001b[39m all_logits\u001b[39m.\u001b[39mshape\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1075\u001b[0m     input_ids,\n\u001b[1;32m   1076\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1077\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1079\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1080\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1082\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1084\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1085\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1087\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[1;32m    890\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    891\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    892\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    893\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    894\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    895\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    896\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    897\u001b[0m     )\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:427\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    426\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 427\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    428\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    429\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:355\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[39m.\u001b[39mFloatTensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m    354\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 355\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(hidden_states)\n\u001b[1;32m    356\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    357\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtanh(math\u001b[39m.\u001b[39msqrt(\u001b[39m2.0\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39mpi) \u001b[39m*\u001b[39m (\u001b[39minput\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.044715\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mpow(\u001b[39minput\u001b[39;49m, \u001b[39m3.0\u001b[39;49m))))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "if algorithm==\"PPO\":\n",
        "    model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=\"./tensorboard_log\")\n",
        "    model.learn(total_timesteps=20000, tb_log_name=\"old_ActionSpace\")\n",
        "    # model.save(\"FirstAgent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/mnt/disks/disk/CS330/STF_CS330_FastGPT/private_support_code/fast-detect-gpt'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
