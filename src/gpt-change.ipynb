{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw5ygzTXOjN1",
        "outputId": "616ec706-3d66-4e94-ba56-7db88ffc10e9"
      },
      "outputs": [],
      "source": [
        "# prompt: install datasets\n",
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk48QDKGP78i",
        "outputId": "23f949f0-2ba2-4451-b963-6fef0121fb1b"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch numpy tqdm openai nltk matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ii-9AgUnt0h"
      },
      "source": [
        "# Fast Detect GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ssCHJWvjFY7P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ziangcao2022/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "\n",
        "def from_pretrained(cls, model_name, kwargs, cache_dir):\n",
        "    local_path = os.path.join(cache_dir, 'local.' + model_name.replace(\"/\", \"_\"))\n",
        "    try:\n",
        "        obj = cls.from_pretrained(local_path, **kwargs)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        obj = cls.from_pretrained(model_name, **kwargs, cache_dir=cache_dir)\n",
        "        obj.save_pretrained(local_path)\n",
        "    return obj\n",
        "\n",
        "# predefined models\n",
        "model_fullnames = {  'gpt2': 'gpt2',\n",
        "                     'gpt2-xl': 'gpt2-xl',\n",
        "                     'opt-2.7b': 'facebook/opt-2.7b',\n",
        "                     'gpt-neo-2.7B': 'EleutherAI/gpt-neo-2.7B',\n",
        "                     'gpt-j-6B': 'EleutherAI/gpt-j-6B',\n",
        "                     'gpt-neox-20b': 'EleutherAI/gpt-neox-20b',\n",
        "                     'mgpt': 'sberbank-ai/mGPT',\n",
        "                     'pubmedgpt': 'stanford-crfm/pubmedgpt',\n",
        "                     'mt5-xl': 'google/mt5-xl',\n",
        "                     'llama-13b': 'huggyllama/llama-13b',\n",
        "                     'llama2-13b': 'TheBloke/Llama-2-13B-fp16',\n",
        "                     'bloom-7b1': 'bigscience/bloom-7b1',\n",
        "                     'opt-13b': 'facebook/opt-13b',\n",
        "                     }\n",
        "float16_models = ['gpt-j-6B', 'gpt-neox-20b', 'llama-13b', 'llama2-13b', 'bloom-7b1', 'opt-13b']\n",
        "\n",
        "def get_model_fullname(model_name):\n",
        "    return model_fullnames[model_name] if model_name in model_fullnames else model_name\n",
        "\n",
        "def load_model(model_name, device, cache_dir):\n",
        "    model_fullname = get_model_fullname(model_name)\n",
        "    print(f'Loading model {model_fullname}...')\n",
        "    model_kwargs = {}\n",
        "    if model_name in float16_models:\n",
        "        model_kwargs.update(dict(torch_dtype=torch.float16))\n",
        "    if 'gpt-j' in model_name:\n",
        "        model_kwargs.update(dict(revision='float16'))\n",
        "    model = from_pretrained(AutoModelForCausalLM, model_fullname, model_kwargs, cache_dir)\n",
        "    print('Moving model to GPU...', end='', flush=True)\n",
        "    start = time.time()\n",
        "    model.to(device)\n",
        "    print(f'DONE ({time.time() - start:.2f}s)')\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(model_name, for_dataset, cache_dir):\n",
        "    model_fullname = get_model_fullname(model_name)\n",
        "    optional_tok_kwargs = {}\n",
        "    if \"facebook/opt-\" in model_fullname:\n",
        "        print(\"Using non-fast tokenizer for OPT\")\n",
        "        optional_tok_kwargs['fast'] = False\n",
        "    if for_dataset in ['pubmed']:\n",
        "        optional_tok_kwargs['padding_side'] = 'left'\n",
        "    else:\n",
        "        optional_tok_kwargs['padding_side'] = 'right'\n",
        "    base_tokenizer = from_pretrained(AutoTokenizer, model_fullname, optional_tok_kwargs, cache_dir=cache_dir)\n",
        "    if base_tokenizer.pad_token_id is None:\n",
        "        base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
        "        if '13b' in model_fullname:\n",
        "            base_tokenizer.pad_token_id = 0\n",
        "    return base_tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7msivRWrn5Fx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "\n",
        "# 15 colorblind-friendly colors\n",
        "COLORS = [\"#0072B2\", \"#009E73\", \"#D55E00\", \"#CC79A7\", \"#F0E442\",\n",
        "            \"#56B4E9\", \"#E69F00\", \"#000000\", \"#0072B2\", \"#009E73\",\n",
        "            \"#D55E00\", \"#CC79A7\", \"#F0E442\", \"#56B4E9\", \"#E69F00\"]\n",
        "\n",
        "\n",
        "def get_roc_metrics(real_preds, sample_preds):\n",
        "    fpr, tpr, _ = roc_curve([0] * len(real_preds) + [1] * len(sample_preds), real_preds + sample_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return fpr.tolist(), tpr.tolist(), float(roc_auc)\n",
        "\n",
        "\n",
        "def get_precision_recall_metrics(real_preds, sample_preds):\n",
        "    precision, recall, _ = precision_recall_curve([0] * len(real_preds) + [1] * len(sample_preds),\n",
        "                                                  real_preds + sample_preds)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    return precision.tolist(), recall.tolist(), float(pr_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iq4ITGPMn9eF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import argparse\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c4GxkxIkoGud"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_samples(logits, labels):\n",
        "    assert logits.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "    nsamples = 10000\n",
        "    lprobs = torch.log_softmax(logits, dim=-1)\n",
        "    distrib = torch.distributions.categorical.Categorical(logits=lprobs)\n",
        "    samples = distrib.sample([nsamples]).permute([1, 2, 0])\n",
        "    return samples\n",
        "\n",
        "def get_likelihood(logits, labels):\n",
        "    assert logits.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "    labels = labels.unsqueeze(-1) if labels.ndim == logits.ndim - 1 else labels\n",
        "    lprobs = torch.log_softmax(logits, dim=-1)\n",
        "    log_likelihood = lprobs.gather(dim=-1, index=labels)\n",
        "    return log_likelihood.mean(dim=1)\n",
        "\n",
        "def get_sampling_discrepancy(logits_ref, logits_score, labels):\n",
        "    assert logits_ref.shape[0] == 1\n",
        "    assert logits_score.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "    if logits_ref.size(-1) != logits_score.size(-1):\n",
        "        # print(f\"WARNING: vocabulary size mismatch {logits_ref.size(-1)} vs {logits_score.size(-1)}.\")\n",
        "        vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n",
        "        logits_ref = logits_ref[:, :, :vocab_size]\n",
        "        logits_score = logits_score[:, :, :vocab_size]\n",
        "\n",
        "    samples = get_samples(logits_ref, labels)\n",
        "    log_likelihood_x = get_likelihood(logits_score, labels)\n",
        "    log_likelihood_x_tilde = get_likelihood(logits_score, samples)\n",
        "    miu_tilde = log_likelihood_x_tilde.mean(dim=-1)\n",
        "    sigma_tilde = log_likelihood_x_tilde.std(dim=-1)\n",
        "    discrepancy = (log_likelihood_x.squeeze(-1) - miu_tilde) / sigma_tilde\n",
        "    return discrepancy.item()\n",
        "\n",
        "def get_sampling_discrepancy_analytic(logits_ref, logits_score, labels):\n",
        "    assert logits_ref.shape[0] == 1\n",
        "    assert logits_score.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "    if logits_ref.size(-1) != logits_score.size(-1):\n",
        "        # print(f\"WARNING: vocabulary size mismatch {logits_ref.size(-1)} vs {logits_score.size(-1)}.\")\n",
        "        vocab_size = min(logits_ref.size(-1), logits_score.size(-1))\n",
        "        logits_ref = logits_ref[:, :, :vocab_size]\n",
        "        logits_score = logits_score[:, :, :vocab_size]\n",
        "\n",
        "    labels = labels.unsqueeze(-1) if labels.ndim == logits_score.ndim - 1 else labels\n",
        "    lprobs_score = torch.log_softmax(logits_score, dim=-1)\n",
        "    probs_ref = torch.softmax(logits_ref, dim=-1)\n",
        "    log_likelihood = lprobs_score.gather(dim=-1, index=labels).squeeze(-1)\n",
        "    mean_ref = (probs_ref * lprobs_score).sum(dim=-1)\n",
        "    var_ref = (probs_ref * torch.square(lprobs_score)).sum(dim=-1) - torch.square(mean_ref)\n",
        "    discrepancy = (log_likelihood.sum(dim=-1) - mean_ref.sum(dim=-1)) / var_ref.sum(dim=-1).sqrt()\n",
        "    discrepancy = discrepancy.mean()\n",
        "    return discrepancy.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yv3QCqJLoJB5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "import json\n",
        "import transformers\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZT_Rz4qpSa-",
        "outputId": "ca236718-c4a8-4c38-a216-c7d9c38fb4ff"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/baoguangsheng/fast-detect-gpt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FROd1DwYpdMw",
        "outputId": "05472a85-b36b-47d6-8ecc-0e14352094a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/disks/disk/CS330/STF_CS330_FastGPT/private_support_code/fast-detect-gpt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ziangcao2022/.local/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "# %cd fast-detect-gpt\n",
        "%cd /home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/private_support_code/fast-detect-gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8N6RGGvpk0R",
        "outputId": "e922a425-7482-4687-c694-e4b2961c631f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LICENSE       \u001b[0m\u001b[01;34mexp_main\u001b[0m/         main_ext.sh       supervised.sh\n",
            "README.md     gpt3to4.sh        requirements.txt  temperature.sh\n",
            "attack.sh     \u001b[01;34mlocal_infer_ref\u001b[0m/  \u001b[01;34mscripts\u001b[0m/          topk.sh\n",
            "\u001b[01;34mexp_gpt3to4\u001b[0m/  main.sh           setup.sh          topp.sh\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lyAErSoqobFT"
      },
      "outputs": [],
      "source": [
        "# reference_model_name = \"gpt-j-6B\"\n",
        "# scoring_model_name = \"gpt-neo-2.7B\"\n",
        "\n",
        "reference_model_name = \"gpt2\"\n",
        "scoring_model_name = \"gpt2\"\n",
        "\n",
        "\n",
        "dataset = \"xsum\"\n",
        "ref_path = \"./local_infer_ref\"\n",
        "device = \"cpu\"\n",
        "cache_dir = \"../cache\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "S2-ooCqSoMzj"
      },
      "outputs": [],
      "source": [
        "class ProbEstimator:\n",
        "    def __init__(self):\n",
        "        self.real_crits = []\n",
        "        self.fake_crits = []\n",
        "        for result_file in glob.glob(os.path.join(ref_path, '*.json')):\n",
        "            with open(result_file, 'r') as fin:\n",
        "                res = json.load(fin)\n",
        "                self.real_crits.extend(res['predictions']['real'])\n",
        "                self.fake_crits.extend(res['predictions']['samples'])\n",
        "        print(f'ProbEstimator: total {len(self.real_crits) * 2} samples.')\n",
        "\n",
        "\n",
        "    def crit_to_prob(self, crit):\n",
        "        offset = np.sort(np.abs(np.array(self.real_crits + self.fake_crits) - crit))[100]\n",
        "        cnt_real = np.sum((np.array(self.real_crits) > crit - offset) & (np.array(self.real_crits) < crit + offset))\n",
        "        cnt_fake = np.sum((np.array(self.fake_crits) > crit - offset) & (np.array(self.fake_crits) < crit + offset))\n",
        "        return cnt_fake / (cnt_real + cnt_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "u1vtHhL9pwkC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class FastDetectGPT:\n",
        "    def __init__(self):\n",
        "        self.device = device\n",
        "        # load model\n",
        "        self.scoring_tokenizer = load_tokenizer(scoring_model_name, dataset, cache_dir)\n",
        "        self.scoring_model = load_model(scoring_model_name, device, cache_dir)\n",
        "        self.scoring_model.eval()\n",
        "        self.reference_model_name = reference_model_name\n",
        "        self.scoring_model_name = scoring_model_name\n",
        "        if self.reference_model_name != self.scoring_model_name:\n",
        "            self.reference_tokenizer = load_tokenizer(self.reference_model_name, dataset, cache_dir)\n",
        "            self.reference_model = load_model(self.reference_model_name, device, cache_dir)\n",
        "            self.reference_model.eval()\n",
        "        # evaluate criterion\n",
        "        self.criterion_name = \"sampling_discrepancy_analytic\"\n",
        "        self.criterion_fn = get_sampling_discrepancy_analytic\n",
        "        self.prob_estimator = ProbEstimator()\n",
        "        # input text\n",
        "        print('Local demo for Fast-DetectGPT, where the longer text has more reliable result.')\n",
        "        print('')\n",
        "\n",
        "    def infer(self, text):\n",
        "        # evaluate text     # (1, 112)\n",
        "        tokenized = self.scoring_tokenizer(text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(self.device)\n",
        "        labels = tokenized.input_ids[:, 1:]\n",
        "        with torch.no_grad():\n",
        "            logits_score = self.scoring_model(**tokenized).logits[:, :-1]\n",
        "            if self.reference_model_name == self.scoring_model_name:\n",
        "                logits_ref = logits_score\n",
        "            else:\n",
        "                tokenized = self.reference_tokenizer(text, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(self.device)\n",
        "                assert torch.all(tokenized.input_ids[:, 1:] == labels), \"Tokenizer is mismatch.\"\n",
        "                logits_ref = self.reference_model(**tokenized).logits[:, :-1]\n",
        "            crit = self.criterion_fn(logits_ref, logits_score, labels)\n",
        "        # estimate the probability of machine generated text\n",
        "        prob = self.prob_estimator.crit_to_prob(crit)\n",
        "        print(f'Fast-DetectGPT criterion is {crit:.4f}, suggesting that the text has a probability of {prob * 100:.0f}% to be fake.')\n",
        "        return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOCRHiiDv1GR",
        "outputId": "003ffb07-c8c1-42a5-d289-8957f038275a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model gpt2...\n",
            "Moving model to GPU...DONE (0.00s)\n",
            "ProbEstimator: total 1800 samples.\n",
            "Local demo for Fast-DetectGPT, where the longer text has more reliable result.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "detector = FastDetectGPT()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pyDRfVaiqMWl"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def model2hfname(model: str) -> str:\n",
        "    return {\n",
        "        \"bert-tiny\": \"prajjwal1/bert-tiny\",\n",
        "        \"bert-med\": \"prajjwal1/bert-medium\",\n",
        "        \"small\": \"gpt2\",\n",
        "        \"med\": \"gpt2-medium\",\n",
        "        \"large\": \"gpt2-large\",\n",
        "        \"full\": \"gpt2-xl\",\n",
        "        \"gpt2-sm\": \"gpt2\",\n",
        "        \"gpt2-med\": \"gpt2-medium\",\n",
        "        \"gpt2-lg\": \"gpt2-large\",\n",
        "        \"gpt2\": \"gpt2-xl\",\n",
        "        \"neo\": \"EleutherAI/gpt-neo-2.7B\",\n",
        "    }[model]\n",
        "\n",
        "def get_model_and_tokenizer(model: str, Cls = transformers.AutoModelForCausalLM, **model_kwargs):\n",
        "    hf_model_name = model2hfname(model)\n",
        "\n",
        "    m = Cls.from_pretrained(hf_model_name, **model_kwargs)\n",
        "    if isinstance(m, transformers.GPT2LMHeadModel):\n",
        "        m.transformer.gradient_checkpointing_enable()\n",
        "\n",
        "    tok = transformers.AutoTokenizer.from_pretrained(hf_model_name)\n",
        "\n",
        "    if tok.pad_token_id is None:\n",
        "        if Cls == transformers.AutoModelForCausalLM:\n",
        "            tok.pad_token = tok.eos_token\n",
        "        else:\n",
        "            print(\"Adding pad token to tokenizer\")\n",
        "            tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "            tok.pad_token = \"[PAD]\"\n",
        "    return m, tok\n",
        "\n",
        "\n",
        "def stop_tokens(tokenizer, stop_string: str = \".\") -> List[int]:\n",
        "    tokens = []\n",
        "    for idx in range(len(tokenizer)):\n",
        "        if tokenizer.decode(idx) == stop_string:\n",
        "            tokens.append(idx)\n",
        "    return tokens\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    values, _ = torch.topk(logits, k)\n",
        "    min_values = values[:, -1]\n",
        "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-iGXWboHqsgs"
      },
      "outputs": [],
      "source": [
        "max_sample_tokens = 10\n",
        "model_name = \"med\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import gymnasium as gym\n",
        "# action_space = gym.spaces.MultiDiscrete([2, 10])\n",
        "# action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(50257,), dtype=np.float32)\n",
        "# observation_space.sample(), observation_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# env.cur_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# env.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pGP7BMLMqAxO"
      },
      "outputs": [],
      "source": [
        "class LMEnv:\n",
        "    def __init__(self, initial_text, sampling_mode: str = \"likelihood\", topK_logistics: int=10):\n",
        "\n",
        "        ## Basic Config\n",
        "        self.max_sample_tokens = max_sample_tokens\n",
        "        self.model, self.tok = get_model_and_tokenizer(model_name)\n",
        "        assert isinstance(self.model, transformers.GPT2LMHeadModel)\n",
        "        self.stop_tokens = stop_tokens(self.tok)\n",
        "        self._seed = None\n",
        "        self.vocab_size = len(self.tok)\n",
        "        # Current inputs and logits\n",
        "        self.initial_text = initial_text\n",
        "\n",
        "\n",
        "        self.topK_logistics = topK_logistics\n",
        "\n",
        "        ## Basic Action Space and Obs Space\n",
        "        # The first integer can take values 0 or 1 (2 possibilities)\n",
        "        # The second integer can take values 1 to 10 (10 possibilities)\n",
        "        import gymnasium as gym\n",
        "        self.action_space = gym.spaces.MultiDiscrete([2, self.topK_logistics])\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.topK_logistics,), dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.sampling_mode = sampling_mode  # \"likelihood\" or \"argmax\"\n",
        "        self.purturb_mode = \"argmax\"\n",
        "\n",
        "        # self.input_ids, self.input_ids_perturb\n",
        "        self.input_ids = None\n",
        "        # self.input_ids_perturb = None\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def _feedforward(self):\n",
        "        # sampled_tokens = []\n",
        "        # cum_logits = []\n",
        "        # n = 0\n",
        "        # cur_input = self.input_ids\n",
        "        # past_kvs = None\n",
        "        # with torch.inference_mode():\n",
        "        #     while n < max_tokens:\n",
        "        #         # print(cur_input.shape)\n",
        "        #         outputs = self.model(cur_input, past_key_values=past_kvs, use_cache=True)\n",
        "        #         local_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        outputs = self.model(self.input_ids, past_key_values=None, use_cache=True)\n",
        "        local_logits = outputs.logits[:, -1, :]\n",
        "        return local_logits\n",
        "    \n",
        "    def _sample_tokens(self, local_logits):\n",
        "        sampled_tokens = []\n",
        "        if self.sampling_mode == \"argmax\":\n",
        "            sampled_token = torch.argmax(local_logits, dim=-1)\n",
        "        elif self.sampling_mode == \"likelihood\":\n",
        "            x = F.softmax(local_logits, dim=-1)\n",
        "            # print(local_logits.shape, x.shape)\n",
        "            sampled_token = torch.multinomial(F.softmax(local_logits, dim=-1), num_samples=1).squeeze(dim=1)\n",
        "            # sampled_token = torch.multinomial(x, num_samples=1).squeeze(dim=1)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        sampled_tokens.append(sampled_token[0])\n",
        "\n",
        "        return sampled_tokens\n",
        "         \n",
        "    def _perturb_tokens(self, local_logits, perturb_ranking):\n",
        "        sampled_tokens = []\n",
        "        # Get the top k predictions （1-10）\n",
        "        topk_values, topk_indices = torch.topk(local_logits, perturb_ranking)\n",
        "        # Select the last item\n",
        "        sampled_tokens.append(topk_values[0][-1])\n",
        "        return sampled_tokens\n",
        "\n",
        "    def _obs_wrapper(self, local_logits):\n",
        "        # Sorted topk_values\n",
        "        topk_values, topk_indices = torch.topk(local_logits, self.topK_logistics)\n",
        "        return topk_values.detach().numpy()\n",
        "\n",
        "    def _cat_new_word(self, sampled_tokens):\n",
        "        return torch.cat((self.input_ids, torch.tensor(sampled_tokens).long().unsqueeze(dim=0)), dim=1)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        ## initial_text\n",
        "        initial_text = self.initial_text\n",
        "        self.input_ids = self.tok(initial_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        ## First 1 step\n",
        "        local_logits = self._feedforward()\n",
        "        self.last_logits = local_logits\n",
        "\n",
        "        sampled_tokens = self._sample_tokens(local_logits)\n",
        "        # TODO: Update the self.ids based ons the ampled_tokens??\n",
        "        self.input_ids = self._cat_new_word(sampled_tokens)\n",
        "\n",
        "        obs = self._obs_wrapper(local_logits)\n",
        "        return obs\n",
        "    \n",
        "    def step(self, action):\n",
        "        # TODO: def step(self, action):\n",
        "        reward = 0\n",
        "\n",
        "        # Parse Action\n",
        "        ## perturb: Binary variable perturb -- either 1 or 0\n",
        "        perturb = action[0]\n",
        "        ## perturb_ranking: 10 options -- shift the choice from 0-9 toward 1-10\n",
        "        perturb_ranking = action[1] + 1\n",
        "\n",
        "        sampled_tokens = self._sample_tokens(self.last_logits)\n",
        "        sampled_output = self._cat_new_word(sampled_tokens)\n",
        "\n",
        "        if not perturb:\n",
        "            self.input_ids = sampled_output\n",
        "        else:\n",
        "            reward -= 1 # Cost of applying perturb\n",
        "            perturbed_tokens = self._perturb_tokens(self.last_logits, perturb_ranking)\n",
        "\n",
        "\n",
        "            perturbed_output = self._cat_new_word(perturbed_tokens)\n",
        "\n",
        "            # Record Scores -- prob\n",
        "            sampled_score = detector.infer(self.tok.decode(torch.squeeze(sampled_output, dim=0)))\n",
        "            perturbed_score = detector.infer(self.tok.decode(torch.squeeze(perturbed_output, dim=0)))\n",
        "\n",
        "            assert sampled_score>=0\n",
        "            assert perturbed_score>=0\n",
        "\n",
        "            reward += (sampled_score-perturbed_score) * 100 # Benefits of applying perturb\n",
        "\n",
        "            self.input_ids = perturbed_output\n",
        "\n",
        "\n",
        "        ## GET NEW OBS\n",
        "        local_logits = self._feedforward()\n",
        "        self.last_logits = local_logits\n",
        "\n",
        "        obs = self._obs_wrapper(local_logits)\n",
        "\n",
        "\n",
        "        info = None\n",
        "\n",
        "\n",
        "        # TODO: Find an appropriate way to update 'done' -- Somehow use your sample_done(self)\n",
        "        # if if sampled_token[0].item() in self.stop_tokens:, done=True\n",
        "        done = False\n",
        "\n",
        "        return obs, reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "    def get_text(self):\n",
        "        return self.tok.decode(torch.squeeze(self.input_ids, dim=0))\n",
        "\n",
        "    def sample_done(self):\n",
        "        return self.input_ids[-1] in self.stop_tokens or self.cur_input.shape[1] >= self.max_sample_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KYoYKicFq3u0"
      },
      "outputs": [],
      "source": [
        "env = LMEnv(\"Here is the start of my new era!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NOmbMUOyrdM3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4342,  318,  262,  923,  286,  616,  649, 6980,    0,  921]])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "D-Wk7HQFrq0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[13, 764]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.stop_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pxNl8Jilr6kF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling...: 100%|██████████| 10/10 [00:01<00:00,  8.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the start of my new era! You\n",
            "\n",
            "Can even add a split in between each\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for idx in tqdm.tqdm(range(10), desc=f\"Sampling...\"):\n",
        "  env.step([0, 0])\n",
        "  # print(env.input_ids)\n",
        "  # print(env.cur_logits)\n",
        "print(env.tok.decode(torch.squeeze(env.input_ids, dim=0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Here is the start of my new era! You\\n\\nCan even add a split in between each'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "crosISwjsoRB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fast-DetectGPT criterion is 1.2470, suggesting that the text has a probability of 45% to be fake.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.45"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detector.infer(env.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Cv0wCQ6jwYhq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling...:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fast-DetectGPT criterion is 0.2564, suggesting that the text has a probability of 12% to be fake.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "OverflowError",
          "evalue": "out of range integral type conversion attempted",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb Cell 32\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     env\u001b[39m.\u001b[39mstep(action\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m(idx)])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     env\u001b[39m.\u001b[39;49mstep(action\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m   \u001b[39m# print(env.input_ids)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m   \u001b[39m# print(env.cur_logits)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(env\u001b[39m.\u001b[39mtok\u001b[39m.\u001b[39mdecode(torch\u001b[39m.\u001b[39msqueeze(env\u001b[39m.\u001b[39minput_ids, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)))\n",
            "\u001b[1;32m/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39m# Record Scores -- prob\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m sampled_score \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39minfer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtok\u001b[39m.\u001b[39mdecode(torch\u001b[39m.\u001b[39msqueeze(sampled_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)))\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m perturbed_score \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39minfer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtok\u001b[39m.\u001b[39;49mdecode(torch\u001b[39m.\u001b[39;49msqueeze(perturbed_output, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39massert\u001b[39;00m sampled_score\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22466173745f475054227d/home/ziangcao2022/workspace/CS330/STF_CS330_FastGPT/src/gpt-change.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39massert\u001b[39;00m perturbed_score\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3746\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3744\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3747\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3748\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3749\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3750\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3751\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(token_ids, \u001b[39mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[39m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mdecode(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[39m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces:\n",
            "\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "for idx in tqdm.tqdm(range(20), desc=f\"Sampling...\"):\n",
        "  if idx % 2:\n",
        "    env.step(action=[0, int(idx)])\n",
        "  else:\n",
        "    env.step(action=[1, 2])\n",
        "  # print(env.input_ids)\n",
        "  # print(env.cur_logits)\n",
        "print(env.tok.decode(torch.squeeze(env.input_ids, dim=0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXUrLlrzBYKO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fast-DetectGPT criterion is -4.6827, suggesting that the text has a probability of 0% to be fake.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detector.infer(env.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
